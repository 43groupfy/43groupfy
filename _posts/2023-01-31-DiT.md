---
layout: post
title:  "[PR]  Scalable Diffusion Models with Transformers (DiT)"
date:   2023-01-31 19:41:25 +0900
categories: diffusion
---

<img src="https://www.wpeebles.com/images/DiT/teaser_v2.png" align='center'>



#### Main Ideas:

1.  Diffusion model backbone으로 U-Net 대신 Transformer를 사용
1.  Transformer의 Scalability가 Diffusion model에서도 있음을 실험으로 보임
1.  가장 효율적인 Conditioning method를 실험을 통해 제안.  



#### Experiment :

* Baselines : LDM + Vit
* Datasets:  ImageNet (256x256, 512x512)
* Metrics: FID, IS, sFID, Precision/Recall
* Quantitative : 



#### Notes

<img src="../assets/img/dit/1.png">



기존 adaLN 방식과 다른점은 MLP Layer를 한번더 통과한다는 점과 Residual connection전에 한번더 scaling을 해준다는 점이다. 

논문에서는  Residual Block의 Identity Initializaition의 학습에 좋다는 이유를 들었다. [[관련 논문]](https://arxiv.org/abs/1802.06093)

개인적으로는 cross attention보다 훨씬 간단한 adaLN의 성능이 더 높은게 신기했다.



<p align='center'>
<img src="../assets/img/dit/2.png", >
<img src="../assets/img/dit/3.png">
</p>

<img src="../assets/img/dit/4.png">

모델의 Scale은 이미지 patch의 크기와 일반적인 Vit모델의 scale parameter을 통해 조절된다. 실험결과들은 전부 scale을 크게 할수록 성능이 증가했음을 보여준다.

<p align='center'>
<img src="../assets/img/dit/5.png">
<img src="../assets/img/dit/6.png">
</p>


<img src="../assets/img/dit/7.png">




#### Thoughts.. 

개인적으로 diffusion model에서 backbone으로 왜 U-net이 사용되야하는지 의문을 갖고있었다. 아무래도 Input과 output의 spatial dimension이 같다는 점에서 착안해

처음에 시도해봤는데 결과가 좋아서, 지속적으로 사용됐던거 같다. 애초에 U-net은 Segmentation을 위해 설계됐기때문에 Transformer로 더 좋은 결과를 만들 수 있을듯??



해당 논문은 단순히 Vit모델을 LDM에 붙이고 몇가지 사소한 모델튜닝만 했으니, Swin Transformer등 모델상의 개선의 여지는 있어보인다. 관련 논문을 찾아봤지만 아직 

diffusion모델에 Transformer backbone을 사용한 논문들은 별로없어보인다. 아무래도 Transformer는 CNN base 모델에 비해 inductive-bias가 없어서 성능을 내려면 

상대적으로  큰 학습데이터가 필요하고 학습부담이 크기 때문인듯. 



ImageNet 말고도 다른 데이터셋에서도 성능이 좋은지 궁금하다.



##### Link 

[[paper]](https://arxiv.org/abs/2212.09748)  [[github]](https://github.com/facebookresearch/DiT) [[project]](https://www.wpeebles.com/DiT)



