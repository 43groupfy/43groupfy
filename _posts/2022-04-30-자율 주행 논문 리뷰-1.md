---
title: 강화학습 자율주행 논문 리뷰-1 (Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings)
author:
  name: Bean
  link: https://github.com/beanie00
date: 2022-04-30 09:32:00 +0800
categories: [Reinforcement learning, Autonomous driving]
tags: [Autonomous driving, paper]
---

`Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings` 와 다음 글에서 다룰 `Autonomous Driving Based on Modified SAC Algotirhm through Imitation Learning Pre-training`은 KAIST 장동의 교수님 연구실에서 작성되었으며 강화학습을 활용해 자율주행 학습을 개선하는 방법을 다루고 있다.

좀 더 구체적으로 이미지 기반의 자율주행 task에서 imitation learning과 reinforcement learning을 합쳐서 함께 사용하여 각각의 장점을 활용하는 방법들을 다룬다. 두 논문의 다른 점은 처음 논문은 강화학습으로 DDPG를 사용하는 반면 두번째 논문은 SAC를 사용한다는 점이다.

## Background
---
* The problem of generating control decision given a monocular camera image and other auxiliary vehicle sensors’ outputs has aroused a lot of interest in recent years.
* Conducting imitation learning towards given human policy might produce a relative well-performed policy
  * However, limitation on the possible policy performance and the potential lack of generalizability for unseen situations.
* Instead, using RL in a simulator and then transfer the learned policy to real world is very popular in recent research.
  * Nonetheless, when the state and action spaces are huge or high dimensional, pure RL algorithm training process usually diverges.
* They present a method which combines these two methods and thus leverage both their advantages for this image-based autonomous driving task.

## Proposed method
---

(1) pre-training(imitation learning) -> (2) fine-tuning(DDPG)
* Network architecture
  <div style="text-align: left">
    <img src="/assets/img/post_images/rl_driving1_1.png" width="70%"/>
  </div>
  * ResNet-34 architecture
    * One of CNN network for deep reinforcement learning tasks like environmental perception, model compression, and object detection
    * Use ResNet-34 as the backbone structure for both actor and critic networks
    * The simple and light ResNet-34 out performs other architectures with attention module integrated in terms of both final testing loss and the generated policy’s performance.

* Imitation learning phase
  * Aim to generate a good initial policy
  * Consider current image input and current vehicle speed as our current state inputs, and throttle, brake from zero to one and steering from minus one to positive one as our control outputs
  * Use Huber loss : to less sensitive to outliers and can make the training process more stable compared to MSE as a result.

* Reinforcement learning phase
  * Reward : distance to the nearest obstacle and current velocity
    <div style="text-align: left">
      <img src="/assets/img/post_images/rl_driving1_2.png" />
    </div>
  * Continuous and deterministic policy setting : to prevent possible catastrophic effect under undesired behavior.
  * In order to combine with imitation learning, they make several changes on the original DDPG
    * actor, critic networks
      * actor network : give all the weights of the pre-trained actor network to the actor networks used in the DDPG.
      * critic network : give all the weights except the final fully connected layer to critic networks used in the DDPG.
    * replay pool : train actor and critic networks using the samples collected under imitation learning’s generated policy. Then, use normal DDPG to collect new experience
    * removing OU noise

## Experiments
---

* Imitation learning phase
  * Use an Xbox controller to control the car under various weather and lighting conditions and record the data using Airsim API
  * 1000 for each episode and terminates when a collision happens.
  * Randomly choose the weather condition and the initial position of the car before each episode starts and calculate a 5 episodes’ average accumulated reward as the criteria.
* RL phase
  * Same setting as imitation learnin phase

* Results
  <div style="text-align: left" >
    <img width="70%" src="/assets/img/post_images/rl_driving1_3.png" />
  </div>
  * Eventually, their proposed method achieves a considerable performance boost from the original imitation learning’s learned policy while the pure DDPG never performs well and does not show any improving trend.
