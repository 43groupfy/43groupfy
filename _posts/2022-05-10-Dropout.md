---
title: Dropout
author:
  name: Bean
  link: https://github.com/beanie00
date: 2022-05-07 16:32:00 +0800
categories: [AI, basic]
tags: []
---

Dropout은 [Prevent Multi layer perceptron Overfitting]()에서와 같이 딥러닝 학습에서 발생하는 문제인 Overfitting을 해소하기 위한 방법 중 하나이다.

## Solution of Overfitting
---
일반적으로 Overfitting을 방지하기 위한 방법은 크게 아래 두가지 방법으로 나뉜다.

* 데이터 수 늘려주기
Overfitting의 원인 중 하나는 데이터의 양이 충분하지 않은 것이다. 데이터의 수가 부족하므로 보편적인 분류를 하지 못하고 훈련데이터에 너무 많은 영향을 받게 된다. 따라서 더 많은 데이터로 학습을 돌려주면 Overfitting을 어느 정도 방지할 수 있다. 하지만 보통의 경우 데이터를 수집하는 것이 매우 비싼 과정이기 때문에 이 방법은 사용하기 어렵다.

* feature 갯수 줄이기
feature의 갯수를 줄이는 것이다. 이들 문제는 multinomial classification에서도 다뤘었는데, 서로 비중이 다른 feature가 섞여서 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나왔었다. 그래서, feature 갯수를 줄이는 것이 중요한데, 이 부분은 deep learning에서는 중요하지 않다고 말씀하셨다. deep learning은 sigmoid 대신 LeRU 함수 계열을 사용하는 것도 있고 바로 뒤에 나오는 dropout을 통해 feature를 스스로 줄일 수 있는 방법도 있기 때문에.

출처: https://pythonkim.tistory.com/42 [파이쿵]

* Regularization
다음으로 Regularization을 사용하여 Overfitting을 방지할 수 있다. [Prevent Multi layer perceptron Overfitting]()에서 소개한 weight decay도 이 Regularization 기법에 포함된다. Regularization은 모델이 학습 데이터에 지나치게 의존하지 않도록 panelty를 부과하는 것이다. 깊은(deep) 모델 일 수록 representation 능력이 너무 좋아 쉽게 Overfitting이 될 수 있다. 이를 방지하기 위해 Regularization 기법들은 일부러 representation 성능에 제한을 주는 것이다. 이 글의 토픽인 Dropout도 Regularization 방법 중 하나이다.

&nbsp;
## Committe Machine
---
Ensemble average

&nbsp;
## Dropout
---
Committe Machine의 예처럼 학습을 할 때, 서로 다른 학습데이터를 통해 모델을 학습시키거나 모델이 서로 다른 구조를 가지면 학습 성능을 개선할 수 있다. 이런 방법을 Model Combination이라고 한다.
하지만 모델이 복잡한 경우 하나의 모델을 훈련시키는 것도 어려운데 여러개의 네트워크를 훈련시키는 것은 매우 어렵다. 또한 이렇게 학습을 시켰다고 해도 이렇게 학습된 모델을 실행할 때도 연산 시간이 오래걸린다.

따라서 Dropout 기법은 모델을 여러개 만들 지 않고 모델 결합이 여러 형태를 가지게 하여 Model Combination과 비슷한 효과를 내게 한다. 네트워크를 학습하는 동안 랜덤하게 일부 뉴런을 생략해버리면, 뉴런의 조합의 지수함수 만큼 다양한 모델을 학습 시키는 것과 비슷해진다. n개의 뉴런이 있다고 하면 $2^{n}$개의 서로 다른 모델이 생성된다.

학습은 이렇게 시켰더라도, 이렇게 학습된 모델을 실행 시킬 때 생성된 $2^{n}$개의 서로 다른 모델을 따로 실행시키면 똑같이 연산 시간이 매우 오래 걸리는 문제가 있다. 따라서 이렇게 많은 모델을 각각 실행하는 것이 아니라, 어짜피 생략된 모델들이 모두 파라미터를 공유하고 있기 때문에 각각의 뉴런들이 모델에서 존재할 (dropout 하지 않을) 확률을 각각의 가중치에 곱해주는 형태로 한 번만 실행을 한다.

정리하면 아래 그림과 같이 학습 시에는 뉴런은 존재할 확률 p로 학습을 진행하고, 실행할 때는 각각의 network에서 얻어진 가중치에 존재 확률 p를 곱해준다.
<div style="text-align: left">
   <img src="/assets/img/post_images/dropout1.png" />
</div>

### Probabilistic apporach
<div style="text-align: left">
   <img src="/assets/img/post_images/dropout2.png" />
</div>
### Combinatorial approach

### Dropout 효과
---
그렇다면 이런 방식으로 어떻게 regulation 효과를 얻을 수 있는 것일까?

* co-adaptation 방지
먼저 학습을 시키다보면, 학습 데이터에 의해 각각의 weight들이 서로
* hidden neuron들의 activity가 좀 더 드문드문(sparsity) 해진다.

### Dropout 효과의 수학적 증명 (single linear unit)
---
### Dropout 효과의 수학적 증명 (single sigmoidal unit)
---
### Dropout 효과의 수학적 증명 (deep neural networks)
---

[https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=laonple&logNo=220818841217](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=laonple&logNo=220818841217)