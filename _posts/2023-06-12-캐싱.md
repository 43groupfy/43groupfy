---
title: 캐싱
layout: post
author: littley
date: 2023-06-07 00:00:00 +0900
categories: [CS, OS]
tags: [OS]
toc: true
toc_sticky: true
img_path: /assets/img/posts/캐싱
---

## 캐싱

캐시를 사용하는 것. 캐싱은 컴퓨터 내부에서 쓰일 뿐만 아니라 운영체제, CDN, DNS 등의 네트워킹 계층, 웹 애플리케이션 및 데이터베이스를 비롯한 다양한 기술 계층에 걸쳐서 활용되고 있다. 

다만 캐싱을 통해 이루고자 하는 목적과 기본적인 원리는 큰 차이가 없기에, 여기서는 캐시와 메인 메모리간의 작업을 중점적으로 다룬다.

> ⚠️ 캐시의 어원(cash ≠ cache)으로는,
> - 물건을 일시적으로 저장, 보관하기 위해 사용하는 곳
> - 기술 용어로서 캐시 - 자주 필요한 데이터의 복사본을 일시적으로 저장, 보관하기 위해 사용하는 곳.

## 캐시 메모리의 도입

![Untitled](Untitled.png)

시간이 지나면서 CPU의 성능은 급속도로 좋아졌지만(80년대와 비교하면 거의 몇만대 ㄷㄷ), 메모리의 성능은 그렇게 좋아지진 않았다. 왜 그럴까? **CPU와 다르게 메모리는 자체 용량을 늘리는 것을 주 목표**로 삼았기 때문이다. 

결과적으로 메모리는 CPU의 데이터 처리 속도를 따라가지 못해서 CPU가 메모리를 기다려야 하는 **병목 현상**이 발생했다. 이를 완화하기 위해 CPU와 메모리 사이를 연결해주는 역할을 하는 것이 캐시다.

앞서 이미 여러번 공부했듯이, 우리는 CPU의 레지스터에서 DISK로 갈수록 용량은 더 크게 보관할 수 있지만, 접근 속도는 느려진다는 점을 이미 알고 있다. 그래서 캐시는 **향후 재사용 가능성이 클 것으로 예상되는 데이터의 복사본을 저장**해두어 CPU가 요청하는 데이터를 바로 전달할 수 있도록 한다.

![DRAM(메인 메모리) vs SRAM(캐시 메모리). 딱 봐도 오른쪽이 구조도 복잡하고 비싸보인다. 메모리는 한 셀당 트랜지스터가 1개, 캐시는 트랜지스터가 6개나 된다고 한다. 또한 물리적으로 차지하는 면적도 SRAM이 크다. ](Untitled%201.png)

DRAM(메인 메모리) vs SRAM(캐시 메모리). 딱 봐도 오른쪽이 구조도 복잡하고 비싸보인다. 메모리는 한 셀당 트랜지스터가 1개, 캐시는 트랜지스터가 6개나 된다고 한다. 또한 물리적으로 차지하는 면적도 SRAM이 크다. 

![Untitled](Untitled%202.png)

이러한 구조에서 캐싱이 CPU와 RAM의 사이에서만 사용되는 것은 아니다. 한 계층은 바로 아래 계층에 대해 캐싱 작업을 수행한다. 레지스터는 캐시 메모리를 캐싱, 캐시는 메인 메모리를 캐싱, 메모리는 디스크를 캐싱한다. 메모리 계층 구조의 목적은 캐싱을 이용해 **빠른 메모리**와 **큰 메모리**의 장점만 결합하여 **크고 빠른 메모리처럼 행동**하게 만드는 것이다.

## 캐시 동작 원리

그렇다면 재사용할 가능성이 클지는 어떻게 알 수 있을까? **데이터 지역성**의 원리를 이용하면 된다. [가상 메모리](https://littley-y.github.io/posts/가상-메모리/) 시간에 공부했던 내용과도 유사하다. 

![Untitled](Untitled%203.png)

왼쪽의 그림은 운영체제가 관리하는 페이지(Page)를 참조한 기록이다. 프로세스는 데이터에 접근할 때 이전에 참조한 데이터와 유사한 주소 영역을 우선적으로 탐색한다.

### 시간 지역성 및 공간 지역성

**시간 지역성**이란 특정 데이터가 한번 접근되었을 경우, 가까운 미래에 또 한번 데이터에 접근할 가능성이 높은 것을 의미한다(ex. for문). 적은 양의 데이터만 캐시에 담을 수 있어 효율적이다.

**공간 지역성**이란 특정 데이터와 가까운 주소에 순서대로 접근하는 경우를 의미한다(ex. 배열). 메모리 주소에 오름차순 혹은 내림차순으로 접근하면 이미 캐시에 저장된 같은 페이지의 데이터에 접근할 수 있으므로 효율적이다.

### 캐시 미스 및 캐시 히트

만약 CPU가 접근한 캐시에 원하는 데이터가 없어 메인 메모리에서 데이터를 가져와야 한다면 이를 **캐시 미스**라고 한다. 캐시는 캐싱 작업을 수행한다. 

반면 원하는 데이터가 있다면 이를 **캐시 히트**라고 한다. 이 때 캐시에 있는 데이터를 읽기 위해 접근한 것이 아니라 캐시에 새로운 데이터를 쓰기 위해 접근한 상태라면, 메인 메모리에 이 업데이트된 데이터를 적용해야 한다. 이를 적용하는 시점에 따라 정책이 두 가지로 나뉜다. 

- Write Through 정책
    
    메인 메모리를 즉시 업데이트한다. 로직이 단순하고 캐시와 메인 메모리의 일관성을 유지할 수 있지만, 매번 데이터가 업데이트 될 때마다 메인 메모리에도 접근해야 하므로 느리다는 단점이 있다.
    
- Write Back 정책
    
    일단 캐시를 업데이트 하고, 업데이트 된 데이터가 캐시에서 빠질 때 메인 메모리를 업데이트 한다.  업데이트 여부를 확인하기 위해 캐시 블록마다 dirty bit를 추가한다. 속도가 빠르지만 캐시와 메모리가 서로 값이 다른 경우가 발생할 수 있다. 
    

## 결론

캐싱이란 : 캐시 메모리에 데이터의 복사본을 저장해 둠으로써 전체적인 처리 속도를 향상시키는 방법.

- 데이터에 직접적으로 접근하는데 시간이 오래 걸릴 때(병목 현상)
- 필요한 값을 얻기 위해 매번 계산하는 과정을 생략하고 싶을 때
- 반복적으로 동일한 결과를 도출해야 할 때(웹의 이미지나 썸네일)

캐시를 사용하게 된다. 주의할 점은 어디까지나 **복사본을 이용**하는 것이므로, 원본과 일관성을 유지시키는 것이 관건이라고 할 수 있겠다!

---

## Reference

[[10분 테코톡] 🏖 파피의 Caching(캐싱)](https://www.youtube.com/watch?v=JBFT4KyEvoY)

[[10분 테코톡] 썬의 캐싱](https://www.youtube.com/watch?v=H4J-8pPMvEU)