<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Prevent Multi layer perceptron Overfitting</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Seize the day" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicons/favicon-32x32.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/Prevent-Multi-layer-perceptron-Overfitting" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="PIGBEAN Tech blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Prevent Multi layer perceptron Overfitting" />
    <meta property="og:description" content="Looking at the existing multi-layer perceptron learning results, it can be seen that the training error continuously decreases as learning progresses. Decreasing error generally means the training is going well, but it may not always be the case. Overfitting may be occuring, where the model follows too closely to the" />
    <meta property="og:url" content="http://localhost:4000/Prevent-Multi-layer-perceptron-Overfitting" />
    <meta property="og:image" content="http://localhost:4000/assets/img/post_images/ai_cover2.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/false" />
    <meta property="article:author" content="https://www.facebook.com/false" />
    <meta property="article:published_time" content="2022-05-04T00:10:00+09:00" />
    <meta property="article:modified_time" content="2022-05-04T00:10:00+09:00" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Prevent Multi layer perceptron Overfitting" />
    <meta name="twitter:description" content="Looking at the existing multi-layer perceptron learning results, it can be seen that the training error continuously decreases as learning progresses. Decreasing error generally means the training is going well, but it may not always be the case. Overfitting may be occuring, where the model follows too closely to the" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/img/post_images/ai_cover2.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="PIGBEAN Tech blog" />
    <meta name="twitter:site" content="@false" />
    <meta name="twitter:creator" content="@false" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
        }
        });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "PIGBEAN Tech blog",
        "logo": "http://localhost:4000/assets/img/favicons/android-chrome-256x256.png"
    },
    "url": "http://localhost:4000/Prevent-Multi-layer-perceptron-Overfitting",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/img/post_images/ai_cover2.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/Prevent-Multi-layer-perceptron-Overfitting"
    },
    "description": "Looking at the existing multi-layer perceptron learning results, it can be seen that the training error continuously decreases as learning progresses. Decreasing error generally means the training is going well, but it may not always be the case. Overfitting may be occuring, where the model follows too closely to the"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Prevent Multi layer perceptron Overfitting" href="/feed.xml" />



</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/"><img src="/assets/img/favicons/android-chrome-256x256.png" alt="PIGBEAN Tech blog" /></a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-getting-started" role="menuitem"><a href="/tag/Catty/">Catty</a></li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 4 May 2022"> 4 May 2022</time>
                    
                </section>
                <h1 class="post-full-title">Prevent Multi layer perceptron Overfitting</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/img/post_images/ai_cover2.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>Looking at the existing multi-layer perceptron learning results, it can be seen that the training error continuously decreases as learning progresses. Decreasing error generally means the training is going well, but it may not always be the case. <code class="language-plaintext highlighter-rouge">Overfitting</code> may be occuring, where the model follows too closely to the training data, and may not be able to perform as well on data outside the training data set.</p>

<h2 id="checking-validation-error">Checking Validation error</h2>
<p> </p>

<p>In order to check the overfitting, validation error should be checked together with training error. <code class="language-plaintext highlighter-rouge">training error</code> refers to errors that occurs during training with the training data, whereas <code class="language-plaintext highlighter-rouge">validation error</code> refers to an error that occurs by predicting new data (validation data). In order to obtain <code class="language-plaintext highlighter-rouge">validation error</code>, the data must first be divided into training dataset and test dataset. This can be done simply by using the <code class="language-plaintext highlighter-rouge">train_test_split</code> function of <code class="language-plaintext highlighter-rouge">sklearn.model_selection</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, let’s calculate the validation error. The error can be calculated with the following function. For the validation data, the error between predicted value using the model trained so far and the actual value was calculated.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ThreeLayerPerceptron_validation_error</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">wOut</span><span class="p">,</span> <span class="n">bOut</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">I</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># 1: input the data
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">[</span><span class="n">I</span><span class="p">]</span>

        <span class="c1"># 2.1: Feed forward
</span>        <span class="n">z1</span> <span class="o">=</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span> <span class="c1"># output layer 1
</span>        <span class="n">z2</span> <span class="o">=</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="c1"># output layer 2
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wOut</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span> <span class="o">+</span> <span class="n">bOut</span><span class="p">)</span>  <span class="c1"># Output of the Output layer
</span>
        <span class="c1"># Append the prediction;
</span>        <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">mu</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">y_valid</span><span class="p">[</span><span class="n">I</span><span class="p">]</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mu</span>
</code></pre></div></div>

<p>Let’s add this validation error to the train code. The same part as the previous code was omitted with ….</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ThreeLayerPerceptron_train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="p">...</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">):</span>
      <span class="n">vec_y</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">I</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>

          <span class="p">...</span>

          <span class="c1"># 4. Computation of the loss function
</span>          <span class="n">mu</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">y_train</span><span class="p">[</span><span class="n">I</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

      <span class="n">epoch_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ThreeLayerPerceptron_validation_error</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">wOut</span><span class="p">,</span> <span class="n">bOut</span><span class="p">))</span>
      <span class="n">validation_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">early_stopping</span><span class="p">.</span><span class="n">validate</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">break</span>

    <span class="p">...</span>

    <span class="k">return</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">wOut</span><span class="p">,</span> <span class="n">bOut</span><span class="p">,</span> <span class="n">mu</span>
</code></pre></div></div>

<p>Plotting the training error and validation error after running training in this way, it can be seen that the training error continues to decrease as shown below, but the validation error increases after a certain section. This is where overfitting occurs.</p>

<div style="text-align: left">
   <img src="/assets/img/post_images/overfitting1.png" width="100%" />
</div>

<p> </p>

<h2 id="prevent-overfitting-adding-early-stopping-term-and-weight-decay-term">Prevent overfitting adding Early stopping term and weight decay term</h2>
<p> </p>

<p>Now that overfitting has been identified, let’s try to prevent it in several ways.</p>

<h3 id="early-stopping">Early stopping</h3>
<p>Early stopping refers to a method of terminating training before the validation error in the graph above enters an increasing trend.</p>

<p>Below is the early stopping implementation code. When the validation error continues to increase 10 times in a row compared to the previous validation error value, learning is terminated.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EarlyStopping</span><span class="p">():</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_prev_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">patience</span>  <span class="o">=</span> <span class="n">patience</span>
  <span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_prev_loss</span> <span class="o">&lt;</span> <span class="n">loss</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">_step</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">_step</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">patience</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Training process is stopped early in {}th epoch</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_prev_loss</span> <span class="o">=</span> <span class="n">loss</span>
</code></pre></div></div>

<h3 id="weight-decay">Weight decay</h3>

<p>Overfitting can be avoided by applying weight decay. When learning, if the learning is carried out simply in the direction in which the loss function becomes smaller, the specific weight values ​​will rather increase and the result may deteriorate. Weight decay exerts a penalty when the weight becomes large in the loss function, so that the weight does not have a too large value during training. L1 regularization and L2 regularization are widely used as the penalty. In this implementation, the L2 method is used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">l2_penalty</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></div>

<p> </p>
<h2 id="final-mlp-training-code">Final MLP training code</h2>
<p> </p>

<p>The final MLP code implementation is as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ThreeLayerPerceptron_train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay_lambda</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

    <span class="c1"># 0: Random initialize the relevant data
</span>    <span class="n">w1</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">p</span> <span class="p">,</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="c1"># Layer 1
</span>    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="n">w2</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">q</span> <span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>  <span class="c1"># Layer 2
</span>    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

    <span class="n">wOut</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>   <span class="c1"># Output Layer
</span>    <span class="n">bOut</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">vec_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">validation_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">):</span>
      <span class="n">vec_y</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">I</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>

          <span class="c1"># 1: input the data
</span>          <span class="n">x</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">I</span><span class="p">]</span>

          <span class="c1"># 2: Start the algorithm
</span>
          <span class="c1"># 2.1: Feed forward
</span>          <span class="n">z1</span> <span class="o">=</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span> <span class="c1"># output layer 1
</span>          <span class="n">z2</span> <span class="o">=</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="c1"># output layer 2
</span>          <span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid_act</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wOut</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span> <span class="o">+</span> <span class="n">bOut</span><span class="p">)</span> <span class="c1"># Output of the Output layer
</span>

          <span class="c1">#2.2: Compute the output layer's error
</span>          <span class="n">delta_Out</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_train</span><span class="p">[</span><span class="n">I</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_act</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">der</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

          <span class="c1">#2.3: Backpropagate
</span>          <span class="n">delta_2</span> <span class="o">=</span> <span class="n">delta_Out</span> <span class="o">*</span> <span class="n">wOut</span> <span class="o">*</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">der</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># Second Layer Error
</span>          <span class="n">delta_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta_2</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="o">*</span> <span class="n">ReLU_act</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">der</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># First Layer Error
</span>
          <span class="c1"># 3: Gradient descent
</span>          <span class="n">wOut</span> <span class="o">=</span> <span class="n">wOut</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">delta_Out</span><span class="o">*</span><span class="n">z2</span> <span class="c1"># Outer Layer
</span>          <span class="n">bOut</span> <span class="o">=</span> <span class="n">bOut</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">delta_Out</span>

          <span class="n">w2</span> <span class="o">=</span> <span class="n">w2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">kron</span><span class="p">(</span><span class="n">delta_2</span><span class="p">,</span> <span class="n">z1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">p</span><span class="p">)</span> <span class="c1"># Hidden Layer 2
</span>          <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span>  <span class="n">eta</span><span class="o">*</span><span class="n">delta_2</span>

          <span class="n">w1</span> <span class="o">=</span> <span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">kron</span><span class="p">(</span><span class="n">delta_1</span><span class="p">,</span> <span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
          <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">weight_decay_lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">delta_1</span>

          <span class="c1"># 4. Computation of the loss function
</span>          <span class="n">mu</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">y_train</span><span class="p">[</span><span class="n">I</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

      <span class="n">epoch_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ThreeLayerPerceptron_validation_error</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">wOut</span><span class="p">,</span> <span class="n">bOut</span><span class="p">))</span>
      <span class="n">validation_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">early_stopping</span><span class="p">.</span><span class="n">validate</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">break</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training error'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_loss</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'validation error'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Averege Loss by epoch'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">wOut</span><span class="p">,</span> <span class="n">bOut</span><span class="p">,</span> <span class="n">mu</span>
</code></pre></div></div>

<p> </p>

<hr />

<h4 id="references-">references :</h4>
<ul>
  <li><a href="https://light-tree.tistory.com/216">https://light-tree.tistory.com/216</a></li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Bean" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/Bean">Beanie</a></h4>
                                
                                    <p>Hello, I’m Beanie, always pondering and crafting services to make life fun and convenient</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/Bean">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://localhost:4000/Prevent-Multi-layer-perceptron-Overfitting';
                            var this_page_identifier = '/Prevent Multi layer perceptron Overfitting';
                            var this_page_title = 'Prevent Multi layer perceptron Overfitting';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://Beanie.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Jekyll-%EA%B8%B0%EC%88%A0%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%97%90-flow-chart%EC%99%80-%EC%88%98%EC%8B%9D-%EB%84%A3%EA%B8%B0">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/jekyll_cover.jpeg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Jekyll-%EA%B8%B0%EC%88%A0%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%97%90-flow-chart%EC%99%80-%EC%88%98%EC%8B%9D-%EB%84%A3%EA%B8%B0">
                <header class="post-card-header">
                    

                    <h2 class="post-card-title">Add flow chart and mathematical expression on Jekyll tech blog (Feat. Mermaid, MathJax)</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Add flow chart using Mermaid
 
Mermaid?

</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Bean/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      2 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/recsys_cover1.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Recommandation system</span>
                            
                        
                            
                               <span class="post-card-tags">Reinforcement learning</span>
                            
                        
                            
                                <span class="post-card-tags">Paper</span>
                            
                        
                    

                    <h2 class="post-card-title">RecSys & RL - A Deep Reinforcement Learning Framework for News Recommendation</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Introduction 딥러닝 모델을 활용한 기존 뉴스 추천 시스템의 취약점 다이나믹하게 변하는 뉴스의 특성과 뉴스에 대한 유저의 선호도 변화를 고려할 때, online learning 이 필요하다. 하지만 기존의 뉴스 추천 시스템은 … 기존 추천 시스템은</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Bean/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
                <img src="/assets/img/favicons/favicon-32x32.png" alt="PIGBEAN Tech blog icon" />
            
            <span>PIGBEAN Tech blog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Prevent Multi layer perceptron Overfitting</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Prevent+Multi+layer+perceptron+Overfitting&amp;url=https://beanie00.github.io/Prevent-Multi-layer-perceptron-Overfitting"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://beanie00.github.io/Prevent-Multi-layer-perceptron-Overfitting"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">PIGBEAN Tech blog</a> &copy; 2022</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-69281367-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
