<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>(Reinforcement learning) Value Function Approximation</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Seize the day" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicons/favicon-32x32.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/(RL)-Value-Function-Approximation" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

    <meta name="google-site-verification" content="X86eN2H5lW1jy6i7OLmOjBAyCf4N8PPVT0sBdIH57LE" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="PIGBEAN Tech blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="(Reinforcement learning) Value Function Approximation" />
    <meta property="og:description" content="이번 글에서는 강화학습 기본 개념 중 Value function approximation에 대해 다루고자 한다. 이 글은 David Silver의 강화학습 강의와 KAIST EE619 강화학습 수업에서 공부한 내용을 바탕으로 작성하였다. Tabular Methods   규모가 작은 MDP 모델의 경우, state와 action을 Table에 저장하여 사용할 수 있다. 더 정확히는 value function을 사용하는 DP 문제의 경우, 모든" />
    <meta property="og:url" content="http://localhost:4000/(RL)-Value-Function-Approximation" />
    <meta property="og:image" content="http://localhost:4000/assets/img/post_images/ai_cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/false" />
    <meta property="article:author" content="https://www.facebook.com/false" />
    <meta property="article:published_time" content="2022-05-13T13:23:00+09:00" />
    <meta property="article:modified_time" content="2022-05-13T13:23:00+09:00" />
    <meta property="article:tag" content="Rl" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="(Reinforcement learning) Value Function Approximation" />
    <meta name="twitter:description" content="이번 글에서는 강화학습 기본 개념 중 Value function approximation에 대해 다루고자 한다. 이 글은 David Silver의 강화학습 강의와 KAIST EE619 강화학습 수업에서 공부한 내용을 바탕으로 작성하였다. Tabular Methods   규모가 작은 MDP 모델의 경우, state와 action을 Table에 저장하여 사용할 수 있다. 더 정확히는 value function을 사용하는 DP 문제의 경우, 모든" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/img/post_images/ai_cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="PIGBEAN Tech blog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Rl" />
    <meta name="twitter:site" content="@false" />
    <meta name="twitter:creator" content="@false" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
        }
        });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "PIGBEAN Tech blog",
        "logo": "http://localhost:4000/assets/img/favicons/android-chrome-256x256.png"
    },
    "url": "http://localhost:4000/(RL)-Value-Function-Approximation",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/img/post_images/ai_cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/(RL)-Value-Function-Approximation"
    },
    "description": "이번 글에서는 강화학습 기본 개념 중 Value function approximation에 대해 다루고자 한다. 이 글은 David Silver의 강화학습 강의와 KAIST EE619 강화학습 수업에서 공부한 내용을 바탕으로 작성하였다. Tabular Methods   규모가 작은 MDP 모델의 경우, state와 action을 Table에 저장하여 사용할 수 있다. 더 정확히는 value function을 사용하는 DP 문제의 경우, 모든"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="(Reinforcement learning) Value Function Approximation" href="/feed.xml" />



</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/"><img src="/assets/img/favicons/android-chrome-256x256.png" alt="PIGBEAN Tech blog" /></a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-getting-started" role="menuitem"><a href="/tag/Catty/">Catty</a></li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-rl post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="13 May 2022">13 May 2022</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/rl/'>RL</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">(Reinforcement learning) Value Function Approximation</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/img/post_images/ai_cover.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>이번 글에서는 강화학습 기본 개념 중 Value function approximation에 대해 다루고자 한다. 이 글은 David Silver의 강화학습 강의와 KAIST EE619 강화학습 수업에서 공부한 내용을 바탕으로 작성하였다.</p>

<h2 id="tabular-methods">Tabular Methods</h2>
<p> </p>

<p>규모가 작은 MDP 모델의 경우, state와 action을 Table에 저장하여 사용할 수 있다. 더 정확히는 value function을 사용하는 DP 문제의 경우, 모든 state s는 V(s) <code class="language-plaintext highlighter-rouge">vector</code>을 가질 것이다. 또한, action-value function을 사용하는 MC, TD의 경우 모든 state-action 쌍 (s, a)에 대하여 Q(s, a) <code class="language-plaintext highlighter-rouge">matrix</code>가 존재한다. 이렇게 행렬로 만들어 푸는 방식을 <strong>Tabular Methods</strong>라고 한다. 하지만 이런 Tabular Method는 state와 action의 차원이 무지하게 커지는 실생활 문제에 Generalization을 할 수 없다.</p>

<p>다음의 문제들을 생각해보자.</p>
<ul>
  <li>Backgammon: $10^{20}$ states</li>
  <li>Compoter Go: $10^{170}$ states</li>
  <li>Robot: continuous state space</li>
</ul>

<p>이런 경우, 특히나 continuous state space를 가지는 경우에는 특히나 Table만으로 핸들링이 불가능하고 따라서 새로운 방법이 필요하다.</p>

<p> </p>
<h2 id="parameterizing-value-function">Parameterizing value function</h2>
<p><br />
 
그렇다면 실생활에 강화학습을 적용하기 위해서는 어떤 방법을 이용해야 할까? state와 action을 table로 구현하는 대신에, $ w$라는 새로운 변수를 통해 <strong>value function</strong>을 함수화함으로써 문제를 해결할 수 있다.
먼저, value function을 function approximation을 이용하여 추정해준다.</p>

\[\hat{V}_{w}(s) \approx V^{\pi}(s) ~~ or ~~\hat{Q}_{w}(s, a) \approx V^{\pi}(s, a)\]

<p>다음으로, seen states(방문한 states)로 부터 unseen states(아직 방문하지 않은 states)로 일반화시킨다.
마지막으로 MC나 TD learning을 이용하여 파라미터 $w$를 학습하면 value function의 optimal한 추정치를 얻을 수 있다.</p>

<p>이를 그림으로 다시 확인해보면,</p>
<div style="text-align: left">
   <img src="/assets/img/post_images/value_approximation.png" width="100%" />
</div>
<p>state가 함수의 input으로 넣고, $w$라는 parameter를 지나 action value function을 output으로 받는 과정이 된다.</p>

<p> </p>
<h2 id="on-policy-prediction-with-function-approximation">On-Policy Prediction with Function Approximation</h2>
<p> </p>
<h3 id="loss-function-mean-square-value-errormsve">Loss Function: Mean-Square Value Error(MSVE)</h3>
<p>On-Policy Prediction problem은 state value function을 추정해나가는 과정이다.
True value function $V^{\pi}(s)$ 와 Approximation function $\hat{V} _{w}(s)$ 가 있을 때, 두 함수가 최대한 같아지도록 $\hat{V} _{w}(s)$ 을 학습시키는 것이 목표이다.
이 때, 두 함수의 차이를 Mean-Square Value Error(MSVE)으로 가늠해볼 수 있다.</p>

\[J(w) = MSE(w) = E_{\pi} = \left [ V^{\pi}(s) - \hat{V}_{w}(s) \right ]^{2} \\ = \sum_{s\in S}^{}\mu^{\pi}(s)\left [ V^{\pi}(s) - \hat{V}_{w}(s) \right ]^{2}\]

<p>여기서 $\mu^{\pi}(s)$ 는 $\pi$ 에 대한 on-policy distribution 이다.</p>

<h3 id="on-policy-distribution">On-Policy Distribution</h3>
<p>MSVE는 두 함수 간의 weighted $L_{2}$ distance 이다. 이떄 각 s에 대해서 weighting importance는 on-policy distribution $d^{\pi}(s)$ 에 의해 주어진다.
$d^{\pi}(s)$ 을 정의하는 한 가지 방법은</p>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>학습은 정의된 MSVE에 대해서 stochastic gradient descent 방법을 통해 진행해준다. Stochastic gradient의 목표는 MSE loss를 최소화하는 vector $w$ 를 찾는 것이다.
Stochastic Gradient Descent을 수행하는 과정은 다음과 같다.</p>
<ol>
  <li>Mean-Square Loss Function: 미분가능한 함수로 주어진다.
    <ul>
      <li>만약 loss function이 미분가능하지 않다면, subgradient를 사용한다.</li>
    </ul>
  </li>
  <li>True Gradient: 먼저 loss function의 true gradient를 계산한다.
    <ul>
      <li>True gradient : $\sum_{s}^{}\mu^{\pi}(s) \left [ \left ( V^{\pi}(s) - \hat{V_{w}}(s) \right) \bigtriangledown \hat{V_{w}}(s) \right ]$</li>
      <li>weight update : $ w_{t+1} = w_{t} + \alpha \sum_{s}^{}\mu^{\pi}(s) \left [ \left ( V^{\pi}(s) - \hat{V_{w}}(s) \right) \bigtriangledown \hat{V_{w}}(s) \right ] $</li>
    </ul>
  </li>
  <li>Sampling the Gradient: 이후 샘플을 통해 stochastic gradient 방법으로 true gradient를 추정한다. 이런 방식을 <code class="language-plaintext highlighter-rouge">sample the gradient</code> 라고 한다. 2번에서 True expectation $\sum_{s\in S}^{}\mu^{\pi}(s)$ 이 <strong>$\mu^{\pi}(s)$ 로 부터 생성된 하나의 샘플 $s_{t}$ 로 계산한 sample mean</strong> 으로 대체된다.
    <ul>
      <li>Stochastic gradient : $\left [ \left ( V^{\pi}(s_{t}) - \hat{V_{w}}(s_{t}) \right) \bigtriangledown \hat{V_{w}}(s_{t}) \right ]$</li>
      <li>weight update : $ w_{t+1} = w_{t} + \alpha \left ( V^{\pi}(s_{t}) - \hat{V_{w}}(s_{t}) \right) \bigtriangledown \hat{V_{w}}(s_{t}) $</li>
    </ul>
  </li>
</ol>

<p>$\mu^{\pi}(s)$로 부터 샘플을 충분히 많이 추출한다면, sample-based SG는 true gradient 로 수렴한다.</p>

<h3 id="on-policy-mc-td-learning">On-Policy MC, TD Learning</h3>
<p>gradient를 계산하기 위해서, $s_{t}, V^{\pi}(s_{t})$ 의 true value가 필요하지만 이 값은 알기 힘들다. 따라서 $V^{\pi}(s_{t})$ 를 추정치인 MC, TD or TD( $\lambda$ ) 추청값으로 대체한다. 각각의 추정치로 대체하면,</p>

<ul>
  <li>Gradient MC for Value Function Estimation: $ w_{t+1} = w_{t} + \alpha \left ( G_{t} - \hat{V_{w}}(s_{t}) \right) \bigtriangledown \hat{V_{w}}(s_{t}) $</li>
  <li>Semi-Gradient TD for Value Function Estimation: $ w_{t+1} = w_{t} + \alpha \left ( r_{t+1} + \gamma \hat{V_{w}}(s_{t+1}) - \hat{V_{w}}(s_{t}) \right) \bigtriangledown \hat{V_{w}}(s_{t}) $</li>
</ul>

<p> </p>
<h2 id="on-policy-control-with-function-approximation">On-Policy Control with Function Approximation</h2>
<p> </p>

<h3 id="prediction-vs-control">Prediction vs Control</h3>
<p>On-Policy Control을 다루기에 앞서 Prediction과 Control의 차이를 복습해보자. Perdiction과 Control의 차이는 policy에 대한 목표와 관련이 있다.</p>

<p>먼저 RL에서 prediction task는 policy가 주어진 상황에서 이 policy가 얼마나 잘 동작하는 지를 측정하는 것을 목표로 한다. 따라서, 주어신 state에서 total reward를 예측하기 위해서 $\pi(\left.\begin{matrix}
a\end{matrix}\right|s)$ 가 고정되어 있다.
반면에 control task는 policy가 고정되어 있지 않으며 최적 policy를 찾는 것을 목표로 한다. 즉, 임의의 주어진 상태에서 total reward의 예측값을 가장 크게 하는 policy $\pi(\left.\begin{matrix}
a\end{matrix}\right|s)$ 를 찾는다.</p>

<h3 id="control-with-value-function-approximation">Control with Value Function Approximation</h3>
<p>Control 과정은 다시 Policy evaluation 과정과 Policy improvement 과정으로 나뉜다. 이 때, model-free를 위해서 action-value function을 사용한다.</p>

<div style="text-align: left">
   <img src="/assets/img/post_images/function_approx1.jpeg" width="80%" />
</div>
<ul>
  <li>Policy evaluation : $\hat{Q}_{w}$ 를 $Q^{\pi}$로 추정</li>
  <li>Policy improvement : action value function에 $ \epsilon - greedy $ 한 action을 취함으로써 improve 진행</li>
</ul>

<h3 id="loss-function-mean-square-value-errormsve-1">Loss Function: Mean-Square Value Error(MSVE)</h3>
<p>Prediction 때와 똑같이 MSVE를 구해보면,</p>

\[J(w) = MSE(w) = E_{\pi} = \left [ Q^{\pi}(s) - \hat{Q}_{w}(s) \right ]^{2} \\ = \sum_{s, a}^{}\mu^{\pi}(s, a)\left [ Q^{\pi}(s, a) - \hat{Q}_{w}(s, a) \right ]^{2}\]

\[- \frac{1}{2} \bigtriangledown  J(w) = \sum_{s, a}^{}\mu^{\pi}(s, a)\left ( Q^{\pi}(s, a) - \hat{Q}_{w}(s, a) \right ) \bigtriangledown \hat{Q}_{w}(s, a)\]

<h3 id="stochastic-gradient-descent-1">Stochastic Gradient Descent</h3>
<p>policy $\pi$ 에서 샘플링한 $(s_{t}, a_{t})$에 대하여 MSVE 값을 이용해서 stochastic gradient descent or semi-gradient를 수행한다.</p>

\[\Delta w =  - \hat{\frac{1}{2}\bigtriangledown J(w)} = (Q^{\pi}(s_{t}, a_{t}) - \hat{Q}_{w_{t}}(s_t, a_{t})) \bigtriangledown \hat{Q}_{w_{t}}(s_t, a_{t})\]

<p>또한, Prediction 마찬가지로 true action value function $ Q^{\pi}(s_{t}, a_{t}) $ 을 알 수 없기 때문에 MC, TD target을 사용한다.</p>
<ul>
  <li>For gradient MC:
 $ w_{t+1} = w_{t} + \alpha \left ( G_{t} - \hat{Q_{w}}(s_{t}, a_{t}) \right) \bigtriangledown \hat{Q_{w}}(s_{t}, a_{t}) $</li>
  <li>For semi-gradient TD:
 $ w_{t+1} = w_{t} + \alpha \left ( r_{t+1} + \gamma \hat{Q_{w}}(s_{t+1}, a_{t+1}) - \hat{Q_{w}}(s_{t}, a_{t+1}) \right) \bigtriangledown \hat{Q_{w}}(s_{t}, a_{t+1}) $</li>
</ul>

<p> </p>
<h2 id="off-policy-prediction-and-control-with-function-approximation">Off-Policy Prediction and Control with Function Approximation</h2>
<p> </p>

<h3 id="off-policy-td-prediction-vpis-importance-sampling">Off-Policy TD Prediction ($V^{\pi}$(s)): Importance Sampling</h3>
<p>Target policy $\pi$, Behavior policy $\beta$ 에 대하여 먼저 Tabular case를 다시 살펴보자.</p>

\[V(s_{t}) \leftarrow V(s_{t}) + \alpha \left (  \frac{\left.\begin{matrix} \pi(a_{t}\end{matrix}\right|s_{t})}{\left.\begin{matrix} \beta(a_{t}\end{matrix}\right|s_{t})} (r_{t+1} + \gamma V(s_{t+1})) - V(s_{t}) \right )\]

\[V(s_{t}) \leftarrow V(s_{t}) + \alpha  \frac{\left.\begin{matrix} \pi(a_{t}\end{matrix}\right|s_{t})}{\left.\begin{matrix} \beta(a_{t}\end{matrix}\right|s_{t})} (r_{t+1} + \gamma V(s_{t+1}) - V(s_{t}))\]

<p>이를 바탕으로 이전과 비슷한 방식으로 Stochastic Gradient Descent을 적용해 weight update 식을 구하면 다음과 같다.</p>

\[w_{t+1} = w_{t} + \alpha \left (  \frac{\left.\begin{matrix} \pi(a_{t}\end{matrix}\right|s_{t})}{\left.\begin{matrix} \beta(a_{t}\end{matrix}\right|s_{t})} (r_{t+1} + \gamma \hat{V}_{w_{t}}(s_{t+1})) - \hat{V}_{w_{t}}(s_{t}) \right ) \bigtriangledown \hat{V}_{w_{t}}(s_{t})\]

\[w_{t+1} = w_{t} + \alpha  \frac{\left.\begin{matrix} \pi(a_{t}\end{matrix}\right|s_{t})}{\left.\begin{matrix} \beta(a_{t}\end{matrix}\right|s_{t})} (r_{t+1} + \gamma \hat{V}_{w_{t}}(s_{t+1}) - \hat{V}_{w_{t}}(s_{t})) \bigtriangledown  \hat{V}_{w_{t}}(s_{t})\]

<p> </p>
<h3 id="off-policy-td-control-action-value-function">Off-Policy TD Control (Action-Value Function)</h3>
<p>Control의 경우에도 Tabular case를 다시 살펴보면,</p>

\[Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha\left ( r_{t+1} + \gamma \sum_{a}^{} \pi(\left.\begin{matrix}
a\end{matrix}\right|s_{t+1})Q(s_{t+1}, a) - Q(s_{t}, a_{t}) \right )\]

<p>Function approximatation의 경우로 발전시키면 다음과 같이 된다.</p>

\[w_{t+1} = w_{t} + \alpha\left ( r_{t+1} + \gamma \sum_{a}^{} \pi(\left.\begin{matrix}
a\end{matrix}\right|s_{t+1}) \hat{Q}_{w_{t}}(s_{t+1}, a) - \hat{Q}_{w_{t}}(s_{t}, a_{t}) \right ) \bigtriangledown  \hat{Q}_{w_{t}}(s_{t}, a_{t})\]

<p> 
<br />
 </p>

<hr />

<p>참고 내용 출처 :</p>
<ul>
  <li>KAIST EE619 Mathmatical Foundations of Reinforcement Learning</li>
  <li><a href="https://stats.stackexchange.com/questions/340462/what-is-predicted-and-controlled-in-reinforcement-learning">https://stats.stackexchange.com/questions/340462/what-is-predicted-and-controlled-in-reinforcement-learning</a></li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Bean" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/Bean">Beanie</a></h4>
                                
                                    <p>Hello, I’m Beanie, always pondering and crafting services to make life fun and convenient</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/Bean">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://localhost:4000/(RL)-Value-Function-Approximation';
                            var this_page_identifier = '/(RL) Value Function Approximation';
                            var this_page_title = '(Reinforcement learning) Value Function Approximation';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://Beanie.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/img/style/bean3.jpg)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; PIGBEAN Tech blog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/rl/">Rl</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Q-learning-&-Double-Q-learning-%EA%B5%AC%ED%98%84">Q-learning & Double Q-learning 구현</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1">RecSys & RL - A Deep Reinforcement Learning Framework for News Recommendation</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/%EC%9E%90%EC%9C%A8-%EC%A3%BC%ED%96%89-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-2">RL - Autonomous Driving Based on Modified SAC Algorithm through Imitation Learning Pre-training</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/rl/">
                                
                                    See all 4 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/%EC%9B%B9%EC%95%B1%EC%97%90%EC%84%9C-%ED%8A%B9%EC%A0%95-%ED%81%AC%EB%A1%AC-%EC%9D%B5%EC%8A%A4%ED%85%90%EC%85%98-%EC%84%A4%EC%B9%98-%EC%97%AC%EB%B6%80-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/catty_cover2.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/%EC%9B%B9%EC%95%B1%EC%97%90%EC%84%9C-%ED%8A%B9%EC%A0%95-%ED%81%AC%EB%A1%AC-%EC%9D%B5%EC%8A%A4%ED%85%90%EC%85%98-%EC%84%A4%EC%B9%98-%EC%97%AC%EB%B6%80-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Catty</span>
                            
                        
                            
                                <span class="post-card-tags">Extension</span>
                            
                        
                    

                    <h2 class="post-card-title">웹앱에서 특정 크롬 익스텐션 설치 여부 확인하기</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Catty 서비스는 크롬 익스텐션과 함께 사용해야 가장 편리하고 효과적으로 사용할 수 있다. 따라서 웹앱에 들어온 사람들 중에 아직 크롬 익스텐션을 설치하지 않은 사람들에게 아래와 같이 크롬 익스텐션을 설치하라는 알림을 띄우고자 하였다.

   


</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Bean/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Q-learning-&-Double-Q-learning-%EA%B5%AC%ED%98%84">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/ai_cover.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Q-learning-&-Double-Q-learning-%EA%B5%AC%ED%98%84">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Rl</span>
                            
                        
                            
                                <span class="post-card-tags">Coding</span>
                            
                        
                    

                    <h2 class="post-card-title">Q-learning & Double Q-learning 구현</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Q-learning과 Double Q-learning
 

</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.png" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Bean/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      7 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
                <img src="/assets/img/favicons/favicon-32x32.png" alt="PIGBEAN Tech blog icon" />
            
            <span>PIGBEAN Tech blog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">(Reinforcement learning) Value Function Approximation</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=%28Reinforcement+learning%29+Value+Function+Approximation&amp;url=https://beanie00.github.io/(RL)-Value-Function-Approximation"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://beanie00.github.io/(RL)-Value-Function-Approximation"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">PIGBEAN Tech blog</a> &copy; 2022</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-69281367-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
