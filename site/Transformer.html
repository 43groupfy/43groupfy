<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Transformer</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Beanie & Kaze's tech blog" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicons/favicon.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/Transformer" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

    <meta name="google-site-verification" content="X86eN2H5lW1jy6i7OLmOjBAyCf4N8PPVT0sBdIH57LE" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Beanie in the wind" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Transformer" />
    <meta property="og:description" content="Transformer은 recurrence나 convolution없이 attention만을 사용하는 새로운 sequence transduction model이다. Transformer은 Attention Is All You Need 라는 논문에서 처음으로 제안되었다. 이 Transformer을 처음 접하고 개념이 복잡해서 유튜브 영상이나 블로그 글들을 찾아보면서 이해를 해보려고 시도했지만 결국에는 이게 처음 제안된 논문을 정독하는 게 가장 도움이 되었다. 그래서 이번 글에서는 Transformer가 제안된 Attention Is" />
    <meta property="og:url" content="http://localhost:4000/Transformer" />
    <meta property="og:image" content="http://localhost:4000/assets/img/post_images/ai_cover2.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/false" />
    <meta property="article:author" content="https://www.facebook.com/false" />
    <meta property="article:published_time" content="2022-05-16T13:02:00+09:00" />
    <meta property="article:modified_time" content="2022-05-16T13:02:00+09:00" />
    <meta property="article:tag" content="Ai" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Transformer" />
    <meta name="twitter:description" content="Transformer은 recurrence나 convolution없이 attention만을 사용하는 새로운 sequence transduction model이다. Transformer은 Attention Is All You Need 라는 논문에서 처음으로 제안되었다. 이 Transformer을 처음 접하고 개념이 복잡해서 유튜브 영상이나 블로그 글들을 찾아보면서 이해를 해보려고 시도했지만 결국에는 이게 처음 제안된 논문을 정독하는 게 가장 도움이 되었다. 그래서 이번 글에서는 Transformer가 제안된 Attention Is" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/img/post_images/ai_cover2.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Beanie in the wind" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Ai" />
    <meta name="twitter:site" content="@false" />
    <meta name="twitter:creator" content="@false" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
        }
        });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Beanie in the wind",
        "logo": "http://localhost:4000/assets/img/favicons/logo.png"
    },
    "url": "http://localhost:4000/Transformer",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/img/post_images/ai_cover2.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/Transformer"
    },
    "description": "Transformer은 recurrence나 convolution없이 attention만을 사용하는 새로운 sequence transduction model이다. Transformer은 Attention Is All You Need 라는 논문에서 처음으로 제안되었다. 이 Transformer을 처음 접하고 개념이 복잡해서 유튜브 영상이나 블로그 글들을 찾아보면서 이해를 해보려고 시도했지만 결국에는 이게 처음 제안된 논문을 정독하는 게 가장 도움이 되었다. 그래서 이번 글에서는 Transformer가 제안된 Attention Is"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Transformer" href="/feed.xml" />



</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/"><img src="/assets/img/favicons/logo.png" alt="Beanie in the wind" /></a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-getting-started" role="menuitem"><a href="/tag/Catty/">Catty</a></li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-ai post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="16 May 2022">16 May 2022</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/ai/'>AI</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">Transformer</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/img/post_images/ai_cover2.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p><strong>Transformer은 recurrence나 convolution없이 attention만을 사용하는 새로운 sequence transduction model</strong>이다. Transformer은 <code class="language-plaintext highlighter-rouge">Attention Is All You Need</code> 라는 논문에서 처음으로 제안되었다.</p>

<p>이 Transformer을 처음 접하고 개념이 복잡해서 유튜브 영상이나 블로그 글들을 찾아보면서 이해를 해보려고 시도했지만 결국에는 이게 처음 제안된 논문을 정독하는 게 가장 도움이 되었다. 그래서 이번 글에서는 Transformer가 제안된 <code class="language-plaintext highlighter-rouge">Attention Is All You Need</code> 을 정리해보기로 하였다.</p>

<p> </p>
<h2 id="background">Background</h2>
<p> </p>

<p>기존의 seq2seq 모델은 인코더-디코더 구조로 구성되어 있었다. 인코더에서는 입력 시퀀스(ex&gt; 문장)를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해 출력 시퀀스를 만들어냈다. 하지만 이 구조에서는 인코더가 입력 시퀀스를 압축하는 과정에서 정보가 일부 손실된다는 단점이 존재했는데, 그래서 이를 보정하기 위하여 어텐션이 사용되었다. 이 때 어텐션은 인코더 쪽의 hidden variable과 디코더 쪽의 hidden variable간의 유사성을 비교하였다. 이 어텐션을 이용하면 입력과 출력 시퀀스의 distance에 대한 고려 없이 둘간의 의존성을 모델링할 수 있다.</p>

<p>본 논문에서는 이렇게 어텐션을 RNN의 단점을 보정하는 역할로서 사용하는 것이 아니라 <strong>어텐션</strong> 만을 사용해 인코더-디코더를 만들어보는 것을 제안한다.</p>

<p> </p>
<h2 id="모델-아키텍쳐">모델 아키텍쳐</h2>
<p> </p>
<div style="text-align: left">
  <img src="/assets/img/post_images/transformer1.jpeg" width="100%" />
</div>

<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>

<h4 id="encoder">Encoder</h4>

<p>트랜스포머는 RNN 없이 인코더-디코더를 구성한다. RNN이 있는 구조에서는 인코더와 디코더에 각각 존재하는 RNN 한 개가 t개의 time step를 가지는 구조였다면, 트랜스포머는 인코더, 디코더 자체가 N개(본 논문에서는 6개)씩 존재한다. 각 6개의 layer은 다시 2개의 sub-layer을 가진다. 여기서 첫 번째 sub-layer은 <strong>multi-head self-attention mechanism</strong>이고, 두번째 sub-layer은 <strong>position-wise fully connected feed-forward 네트워크</strong> 이다. 또한, Encoder에서 layer nomalization을 수행한 이후에 각 sub-layers에서 residual connection을 해준다.</p>

<h4 id="decoder">Decoder</h4>
<p>Encoder와 마찬가지로 6개의 layer을 stack하였고, 동일한 2개의 sub-layer을 가지며 이에 떠해 multi head attention을 수행하는 3번째 sub-layer가 추가된다. 또한 마찬가지로 layer nomalization 이후 각 sub-layer에서 residual connection을 해준다.</p>

<p>반면 Encoder와는 달리 self-attention을 수행하는 sub-layer에 조금 수정을 가했는데,</p>

<h3 id="attention">Attention</h3>

<p>Attention 함수는 query와 key-value pair (key, value) 의 집합을 output으로 매핑한다. 좀 더 자세히 말하면, output은 value들의 weighted sum으로 계산되는 데, 이 때, 각 value에 대응되는 weight 값은 query와 대응되는 key간의 compatibility function으로 계산된다.</p>

<div style="text-align: left">
  <img src="/assets/img/post_images/transformer2.jpeg" width="100%" />
</div>

<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>

<p>Compatibility 함수로 대표적으로 <strong>scaled dot product</strong> 를 사용한다. Scaled dot product에서 각 value의 weight은 다음 식으로 계산된다.</p>

\[Attention(Q, K, V) = softmax( \frac{QK^{T}}{\sqrt{d_{k}}} ) V\]

<p>이 때, 식을 확인해보면 $ \frac{1}{\sqrt{d_{k}}} $ 를 추가로 곱해주는 것을 볼 수 있다. 이는 $ d_{k} $ 가 큰 수 일 때, dot product가 너무 큰 값을 가져 softmax function이 매우 작은 gradient 값을 가지게 된다. 따라서 dot product를 $ d_{k} $ 로 나눠 이를 보정해준다.</p>

<h4 id="multi-head-attention">Multi-Head Attention</h4>
<p>어텐션을 큰 dimension에 대하여 한 번에 적용하면 학습 속도가 오래 걸린다. 따라서 본 논문에서는 어텐션을 사용할 때 한 번에 적용하는 것이 아니라 여러개로 분할하여 병렬로 수행한 뒤 이를 다시 하나로 합치는 방법을 제안하였다. 즉, 전체 dimension을 h로 나눠서 병렬로 attention을 h번 적용한다.</p>

<ol>
  <li>query와 key, value를 각각 $ d_{k}, d_{k}, d_{v} $ 차원에 linear projection한다.</li>
  <li>이렇게 projection 된 버전의 query, key, value를 가지고 Scaled dot product attention을 수행한다.</li>
  <li>이 과정을 h번 반복한다.</li>
  <li>구해진 h개의 벡터를 합치고 다시 project 하여 최종 값을 얻는다.</li>
</ol>

<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>
<p>sub layer에 적용되는 attention에 추가로, 인코더, 디코더의 각 레이어는 각 position에 동일하게 적용되는 fully connected feed-forward network를 가진다. 이 네트워크는 ReLu activation을 사이에 둔 두개의 linear transformation으로 구성된다.</p>

\[FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}\]

<h3 id="embeddings-and-softmax">Embeddings and Softmax</h3>
<p>다른 sequence transduction 모델과 유사하게, 입력 토큰들을 출력 토큰들로 변환해주는 데 학습된 embedding을 사용한다. 또한, 디코더 출력을 다음 토큰의 확률로 바꿔주는데 학습된 transformation과 softmax 함수를 사용한다.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>트랜스포머 모델에서는 recurrence도, convolution도 사용하지 않는다. 따라서 모델이 시퀀스의 순서 정보를 처리하기 위해서는 시퀀스에 있는 각 토큰의 상대 위치 혹은 절대 위치 정보를 추가로 알려줘야 한다. 이를 위해서 인코더, 디코더에 들어가기 전에 각각의 하위 레이어인 임베딩 레이어에 <strong>positional encoding</strong>이 추가하여 sequence에 순서를 준다.</p>

<p>positional encoding는 다양한 방법으로 구할 수 있는데, 논문에서는 다양한 주파수의 사인, 코사인 함수를 사용하여 이 값을 계산한다.</p>

<p>이처럼 positional encoding을 사용하면 시퀀스의 순서 정보를 보존할 수 있게 된다. 이후 인코더와 디코더에서는 같은 단어를 입력 받아도 순서 정보에 따라 다른 벡터 값을 처리하게 된다.</p>

<p> </p>
<h2 id="training">Training</h2>
<p> </p>

<h3 id="training-data-and-batching">Training Data and Batching</h3>
<p>학습에는 4.5백만개의 setence pair로 구성된 standard WMT 2014 English-German dataset을 사용하였다 또한, English-French 학습에는 보다 큰 (36M) WMT 2014 English-Frensh dataset을 사용하였다. sentence pair은 길이에 따라 batch로 묶었고, 각 batch는 약 25000개의 source token과 약 25000개의 target token으로 구성되어 있다.</p>

<h3 id="optimizer">Optimizer</h3>
<p>Adam optimizer을 사용하였다.</p>

<h3 id="regularization">Regularization</h3>
<p>정규화 방식으로 Residual dropout을 사용하였다.</p>

<p> </p>
<h2 id="result">Result</h2>
<p> </p>

<ul>
  <li>Base Model만으로도 충분히 최고 성능을 보였으며,</li>
  <li>특히 Big Model의 경우 state-of-the-art를 상당한 수준으로 갱신하는 성능을 보여주었다.
    <ul>
      <li>WMT 2014 English to German 번역 과제에서 big transformer 모델이 앙상블 모델을 포함한 기존 모델 보다 2.0 BLEU 이상으로 더 좋은 성능을 내었다.</li>
      <li>WMT 2014 English to French 번역 과제에서 big transformer 모델이 이전의 다른 single model보다 training 비용은 1/4로 줄었음에도 BLEU는 더 좋았다.</li>
    </ul>
  </li>
</ul>

<p> </p>
<h2 id="conclusion">Conclusion</h2>
<p> </p>

<p>많이 언급하였듯이, Transformer는 recurrence와 convolution을 모두 제거한, 오직 attention에만 의존하는 새로운 종류의 모델이다. 이 모델은 계산량을 줄이고 병렬화를 적용해 학습 속도가 훨씬 빠를 뿐만 아니라 그 성능 또한 state-of-the-art를 달성하는 수준에 이르렀다. 또한 이러한 attention에 기반한 모델은 다른 task들에 적용할 수도 있다. 비단 텍스트뿐만 아니라 이미지, 오디오나 비디오 등의 상대적으로 큰 입력-출력을 요하는 task들에 효과적으로 사용할 수 있을 것이다.</p>

<p><br />
 </p>

<hr />

<p>참고 내용 출처 :</p>
<ul>
  <li>https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/</li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/img/style/beanie.svg" alt="Beanie" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/Beanie">Beanie</a></h4>
                                
                                    <p>Hello, I’m Beanie, always pondering and crafting services to make life fun and convenient</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/Beanie">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://localhost:4000/Transformer';
                            var this_page_identifier = '/Transformer';
                            var this_page_title = 'Transformer';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://Beanie.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/img/style/sky.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Beanie in the wind &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/ai/">Ai</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Implement-Bidirectional-Associative-Memory">Bidirectional Associative Memory 구현</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Dropout">Dropout (Mathmatical approach)</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/Implement-Multi-layer-perceptron-(from-scratch)">Implement Multi layer perceptron (from scratch)</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/ai/">
                                
                                    See all 6 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/RecSys-&-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/recsys_cover1.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/RecSys-&-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Recsys</span>
                            
                        
                            
                               <span class="post-card-tags">Rl</span>
                            
                        
                            
                                <span class="post-card-tags">Paper</span>
                            
                        
                    

                    <h2 class="post-card-title">(작성중) RecSys & RL - Top-𝐾 Off-Policy Correction for a REINFORCE Recommender System</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>이전에 읽었던 A Deep Reinforcement Learning Framework for News Recommendation 논문은 알고리즘 구현 코드를 찾기가 어려웠다. 그래서 알고리즘 코드가 같이 있으면서 적당히 challenging한 논문을 다시 찾아보았고 이 Top-𝐾 Off-Policy Correction for a REINFORCE</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.svg" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Beanie/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      2 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Implement-DQN">
                <div class="post-card-image" style="background-image: url(/assets/img/post_images/ai_cover.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Implement-DQN">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Rl</span>
                            
                        
                            
                                <span class="post-card-tags">Coding</span>
                            
                        
                    

                    <h2 class="post-card-title">Deep Q-Network(DQN) 구현</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>  DQN 이란?   이전에 (Reinforcement learning) Value Function Approximation 포스팅에서 Value function approximation에 대하여 다루었다. 요약하면 강화학습에서 간단한 table 형태로 학습을 하게 되면 학습이 극도로 느려지는 문제가 있어 value function을 다양하게 근사하여</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                        
                        <img class="author-profile-image" src="/assets/img/style/beanie.svg" alt="Beanie" />
                        
                        <span class="post-card-author">
                            <a href="/author/Beanie/">Beanie</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      8 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
                <img src="/assets/img/favicons/logo.png" alt="Beanie in the wind icon" />
            
            <span>Beanie in the wind</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Transformer</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Transformer&amp;url=https://beanie00.github.io/Transformer"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://beanie00.github.io/Transformer"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">Beanie in the wind</a> &copy; 2022</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-69281367-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
